{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670a6ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0\n",
      "Is MPS (GPU) available? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Is MPS (GPU) available? {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b73ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n"
     ]
    }
   ],
   "source": [
    "class_idx = {1: \"World\" , 2: \"Sports\" , 3: \"Business\" , 4: \"Sci/Tech\"}\n",
    "print(class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d7cf8",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6409304",
   "metadata": {},
   "source": [
    "### Choose and Load Tools (The Model and its Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b336abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac889076b8dd4d8aa2787e714cfe4359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3a9cfadd654c93865098933f45ad02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5755235ed3b24b47aacf610b902ee8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c187de84bab4abe9a9281d910ecb4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce4865efa5a4590a125068ed874ab09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel , AutoTokenizer\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# Successfully downloaded the specific \"vocabulary\" and \"grammar rules\" for the distilbert-base-uncased model.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') \n",
    "# Downloaded the pre-trained \"master chef\" model itself, with all its existing knowledge of the English language.\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3fc77a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc7cb7c84194f0b861bae0c8d5573e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c06cc4648bb4c9cba286b3f66563e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded local CSV files:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Class Index', 'Title', 'Description'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Class Index', 'Title', 'Description'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "from datasets import load_dataset, DatasetDict\n",
    "train_csv = \"/Users/tushar04master/Documents/news-classifier/data/train.csv\"\n",
    "test_csv = \"/Users/tushar04master/Documents/news-classifier/data/test.csv\"\n",
    "\n",
    "# Using Dataset library\n",
    "ag_news_dataset = load_dataset('csv', data_files={'train': train_csv, 'test': test_csv})\n",
    "print(\"Successfully loaded local CSV files:\")\n",
    "print(ag_news_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90552b",
   "metadata": {},
   "source": [
    "### Create a Tokenization \"Worker\" Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b4700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Worker Function\n",
    "def tokenize_fxn(examples):\n",
    "     \"\"\"\n",
    "    Takes a batch of text from the dataset and applies the tokenizer.\n",
    "    \"\"\"\n",
    "     return tokenizer(\n",
    "          examples[\"Description\"],\n",
    "          truncation = True,\n",
    "          max_length = 256\n",
    "     )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d735de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f13b94b9924286a67d5c7ada128082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f926350537e4507917a44a34a287cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "#Apply Your Worker to the Entire Dataset\n",
    "print(\"\\nTokenizing the dataset...\")\n",
    "# The .map() function runs our tokenize_function on every example.\n",
    "# batched=True makes the process much faster by working on chunks at a time.\n",
    "tokenized_datasets = ag_news_dataset.map(tokenize_fxn, batched=True)\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d039380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of a tokenized data point:\n",
      "{'Class Index': 3, 'Title': 'Wall St. Bears Claw Back Into the Black (Reuters)', 'Description': \"Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'input_ids': [101, 26665, 1011, 2460, 1011, 19041, 1010, 2813, 2395, 1005, 1055, 1040, 11101, 2989, 1032, 2316, 1997, 11087, 1011, 22330, 8713, 2015, 1010, 2024, 3773, 2665, 2153, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample of a tokenized data point:\")\n",
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6566d4f",
   "metadata": {},
   "source": [
    "# That's Cool ðŸ˜™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
